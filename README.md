# Data Engineering Using Azure Databricks: Formula 1

![Formula 1](https://images2.alphacoders.com/121/thumb-1920-1214054.png)

## ğŸš€ Project Overview
This project aims to provide a data analysis solution for Formula 1 race results using **Azure Databricks**. The ETL pipeline ingests Formula 1 motor racing data, transforms it, and loads it into a data warehouse for reporting and analysis. The data is sourced from **ergast.com** and stored in **Azure Data Lake Gen2**. Data transformation and analysis are performed using **Azure Databricks**, and the process is orchestrated using **Azure Data Factory**.

---

## ğŸï¸ Formula 1 Overview
Formula 1 (F1) is the premier international racing series governed by the **FIA**. It consists of **10 teams**, each with **two drivers**, competing in **20-23 races (Grands Prix)** per season. The driver with the highest points at the end of the season is crowned the **Drivers' Champion**, while the best-performing team wins the **Constructors' Championship**.

### Key Features of F1:
- **Hybrid-engine, high-tech cars** ğŸï¸
- **50-70 lap races** across various countries ğŸŒ
- **Pit stops** for tire changes and adjustments ğŸ”§
- **Grid positions** determined by qualifying rounds âš¡

---

## ğŸ“Š Architecture Diagram
![Solution Architecture](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/ETL_Architecture.png)

---

## ğŸ“œ ER Diagram
Database structure based on the **Ergast Developer API**:

![ER Diagram](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/formula1_db_data_model.png)

For detailed documentation, visit the [Database User Guide](http://ergast.com/docs/f1db_user_guide.txt).

---

## âš™ï¸ How It Works
### ğŸ“‚ Source Data Files
Data is sourced from **Ergast Developer API**, covering race results from **1950 to 2022**.

| File Name   | File Type               |
|------------|-------------------------|
| Circuits   | CSV                     |
| Races      | CSV                     |
| Constructors | JSON (Single Line)     |
| Drivers    | JSON (Nested)           |
| Results    | JSON (Single Line)      |
| PitStops   | JSON (Multi Line)       |
| LapTimes   | CSV (Split Files)       |
| Qualifying | JSON (Multi Line Split) |

### ğŸ”„ Execution Overview
- **Bronze Zone**: Raw data ingestion using **Azure Data Factory (ADF)**.
- **Silver Zone**: Data transformation into **Delta Tables** using **Azure Databricks**.
- **Gold Zone**: Aggregated data for **reporting and analytics**.
- **ETL Pipelines** handle:
  - **Ingestion** (Bronze â†’ Silver)
  - **Transformation** (Silver â†’ Gold)

---

## ğŸ—ï¸ Azure Resources Used
- **Azure Data Lake Storage** ğŸ“
- **Azure Data Factory** ğŸ”„
- **Azure Databricks** ğŸ”¥
- **Azure Key Vault** ğŸ”‘

---

## ğŸ“Œ Project Requirements
### 1ï¸âƒ£ Data Ingestion
âœ… Ingest all 8 files into **Azure Data Lake**  
âœ… Apply **schema** and **audit columns**  
âœ… Store data in **Delta format**  
âœ… Enable **incremental loading**  

### 2ï¸âƒ£ Data Transformation
âœ… Join tables for **reporting** & **analysis**  
âœ… Apply **audit columns**  
âœ… Store data in **Delta format**  
âœ… Handle **incremental loads**  

### 3ï¸âƒ£ Data Reporting
âœ… **Driver Standings** ğŸ“Š  
âœ… **Constructor Standings** ğŸï¸  

### 4ï¸âƒ£ Data Analysis
âœ… Identify **dominant drivers & teams** ğŸ†  
âœ… Visualize **outputs in Databricks and PowerBI Dashboards** ğŸ“ˆ  

### 5ï¸âƒ£ Scheduling
âœ… **Automated pipeline execution** every **Sunday at 10 PM** â³  
âœ… **Monitoring & alerts** for failures ğŸ””  
âœ… **Rerun failed pipelines** â™»ï¸  

### 6ï¸âƒ£ Non-Functional Requirements
âœ… Ability to **delete individual records**  
âœ… Ability to **see history and time travel** âª  
âœ… Ability to **roll back to a previous version** ğŸ”„  

---

## ğŸ“‰ Analysis Results
![Result1](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/PowerBI/F1_Dashboard.png)

---

## Setup Using Service Principal  
ğŸ“Œ *Follow the setup guide in the [Setup Folder](set-up/Mount ADLS.py)*  

![Service Principal Setup](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/Set_up.png)  

---

## Incremental Load Process  
ğŸ”¹ **Day 1 (Cutover Date)**  
![Incremental Load - Day 1](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/IL_d1.png)  

ğŸ”¹ **Day 2**  
![Incremental Load - Day 2](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/IL_d2.png)  

ğŸ”¹ **Day 3**  
![Incremental Load - Day 3](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/resource/IL_d3.png)  

---

## Azure Data Factory (ADF) Pipelines  
ğŸ“Œ **ADF Components Overview**  
![ADF Components](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/ADF_Pipeline_Automation/ADF%20components.png)  

ğŸ“Œ **Formula 1 Data Pipeline**  
![F1 Pipeline](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/ADF_Pipeline_Automation/End_To_End_Automation.png)  

ğŸ“Œ **F1 Pipeline Run Execution**  
![F1 Pipeline Run](https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1/blob/main/ADF_Pipeline_Automation/Pipeline%20Runs.png)  

---

## Technologies/Tools Used  
- Pyspark  
- Spark SQL  
- Delta Lake  
- Azure Databricks  
- Azure Data Factory  
- Azure Data Lake Storage Gen2  
- Azure Key Vault  
- Power BI  

---

## ğŸ“Œ Getting Started
### Prerequisites
- Azure subscription  
- Databricks workspace  
- Azure Data Factory  

### Setup Instructions
1. Clone the repository:
   ```bash
   git clone https://github.com/Aryan-Nikul-Patel/Data-Engineering-Using-Azure-Databricks-Formula-1.git
   ```
2. Configure **Azure Databricks** and **ADF**.
3. Set up **Azure Key Vault** for security.
4. Run the **ETL pipeline** and start analyzing!

---

## ğŸ“œ Conclusion
This project leverages **Azure Databricks** to process **Formula 1 race data** efficiently. By implementing an optimized **ETL pipeline**, it enables **data-driven insights**, analytics, and reporting for F1 statistics.

ğŸš€ If you found this project useful, **give it a â­ on GitHub!** ğŸ‰

---

## ğŸ¤ Connect with Me
ğŸ”— [LinkedIn](https://www.linkedin.com/in/aryan-nikul-patel/)  
ğŸ“§ Email: **anpnhp2002@gmail.com**